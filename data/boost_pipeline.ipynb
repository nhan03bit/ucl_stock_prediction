{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3027bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (354089, 6)\n",
      "Columns: ['price', 'postcode_mean_price', 'street_mean_price', 'town_city_mean_price', 'district_mean_price', 'date_of_transfer']\n",
      "Date range: 2025-04-01 00:00:00 to 2025-11-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suppress warnings for cleaner notebook output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Wrapper for CatBoost to make it compatible with sklearn pipelines\n",
    "class CatBoostRegressorWrapper(CatBoostRegressor, BaseEstimator, RegressorMixin):\n",
    "    pass  # Inherits methods; BaseEstimator provides __sklearn_tags__\n",
    "\n",
    "# Load high correlation features data\n",
    "data = pd.read_csv('preprocessed_real_estate_high_corr.csv')\n",
    "\n",
    "# Load original data to get date_of_transfer\n",
    "original_data = pd.read_csv('real_estate.csv')\n",
    "original_data['date_of_transfer'] = pd.to_datetime(original_data['date_of_transfer'])\n",
    "original_data = original_data[(original_data['date_of_transfer'] >= '2025-04-01') & (original_data['date_of_transfer'] <= '2025-12-31')]\n",
    "\n",
    "# Add date_of_transfer to preprocessed data (matching indices)\n",
    "data['date_of_transfer'] = original_data['date_of_transfer'].values[:len(data)]\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Columns: {data.columns.tolist()}\")\n",
    "print(f\"Date range: {data['date_of_transfer'].min()} to {data['date_of_transfer'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af299d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: price\n",
      "Features: ['postcode_mean_price', 'street_mean_price', 'town_city_mean_price', 'district_mean_price']\n",
      "Total features: 4\n",
      "Data sorted by date_of_transfer\n"
     ]
    }
   ],
   "source": [
    "# Sort by date\n",
    "data = data.sort_values('date_of_transfer').reset_index(drop=True)\n",
    "\n",
    "# Define target\n",
    "TARGET = 'price'\n",
    "\n",
    "# Define features (all except target and date)\n",
    "feature_cols = [col for col in data.columns if col != TARGET and col != 'date_of_transfer']\n",
    "\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"Data sorted by date_of_transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e183c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size: 354089\n",
      "Training samples: 283271, Test samples: 70818\n",
      "Feature matrix shape: (354089, 4)\n",
      "Target shape: (354089,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================================\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data[TARGET]\n",
    "\n",
    "# Train-test split (80-20)\n",
    "train_size = int(0.8 * len(data))\n",
    "X_train = X.iloc[:train_size]\n",
    "X_test = X.iloc[train_size:]\n",
    "y_train = y.iloc[:train_size]\n",
    "y_test = y.iloc[train_size:]\n",
    "\n",
    "print(f\"\\nDataset size: {len(data)}\")\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f3be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset size after feature engineering: 354089\n",
      "Training samples: 283271, Test samples: 70818\n"
     ]
    }
   ],
   "source": [
    "X = data[feature_cols]\n",
    "y = data[TARGET]  # Single output target\n",
    "\n",
    "# Train-test split (shuffle=False for time-series)\n",
    "train_size = int(0.8 * len(data))\n",
    "X_train = X.iloc[:train_size]\n",
    "X_test = X.iloc[train_size:]\n",
    "y_train = y.iloc[:train_size]\n",
    "y_test = y.iloc[train_size:]\n",
    "\n",
    "print(f\"\\nDataset size after feature engineering: {len(data)}\")\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d2ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models (single output regressors)\n",
    "models = {\n",
    "    'GradientBoosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', GradientBoostingRegressor(random_state=42))\n",
    "    ]),\n",
    "    'LightGBM': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LGBMRegressor(random_state=42, verbose=-1))\n",
    "    ]),\n",
    "    'CatBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', CatBoostRegressorWrapper(random_state=42, verbose=0))\n",
    "    ]),\n",
    "    'AdaBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', AdaBoostRegressor(random_state=42))\n",
    "    ]),\n",
    "    'XGBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', XGBRegressor(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a83767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models, X_train, y_train):\n",
    "    trained_models = {}\n",
    "    for name, pipeline in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        trained_models[name] = pipeline\n",
    "    return trained_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be62c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, pipeline in models.items():\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        results[name] = mse\n",
    "        print(f\"{name} MSE: {mse:.4f}\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aab326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "GradientBoosting MSE: 98168874195.7867\n",
      "LightGBM MSE: 385257831765.5060\n",
      "CatBoost MSE: 388055293922.5231\n",
      "AdaBoost MSE: 154434799153.1350\n",
      "XGBoost MSE: 383339598818.7050\n"
     ]
    }
   ],
   "source": [
    "# Train on full training data\n",
    "from xml.parsers.expat import model\n",
    "\n",
    "\n",
    "trained_models = train_models(models, X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_models(trained_models, X_test, y_test)\n",
    "\n",
    "# ============================================================\n",
    "# MULTI-HORIZON FORECASTING\n",
    "# ============================================================\n",
    "\n",
    "horizons = [1, 3, 5, 7, 14, 30]  # days ahead to predict\n",
    "\n",
    "def prepare_horizon_data(X, y, horizon):\n",
    "    y_horizon = y.shift(-horizon)\n",
    "    X_horizon = X.copy()\n",
    "    valid_idx = ~y_horizon.isna()\n",
    "    X_horizon = X_horizon[valid_idx]\\\n",
    "    y_horizon = y_horizon[valid_idx]\n",
    "    train_size = int(0.8 * len(X_horizon))\n",
    "    return X_horizon.iloc[:train_size], X_horizon.iloc[train_size:], y_horizon.iloc[:train_size], y_horizon.iloc[train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e234a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Horizon: 1 days ===\n",
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "GradientBoosting MSE: 603614470762.5846\n",
      "LightGBM MSE: 714016211464.9281\n",
      "CatBoost MSE: 1278477933787.1687\n",
      "AdaBoost MSE: 7806559792114.2334\n",
      "XGBoost MSE: 1565314490115.0593\n",
      "\n",
      "=== Horizon: 3 days ===\n",
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "GradientBoosting MSE: 636922751034.5118\n",
      "LightGBM MSE: 698037567090.0652\n",
      "CatBoost MSE: 803182815846.1000\n",
      "AdaBoost MSE: 14425543726441.1055\n",
      "XGBoost MSE: 9551341097289.6094\n",
      "\n",
      "=== Horizon: 5 days ===\n",
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "GradientBoosting MSE: 586986025374.2119\n",
      "LightGBM MSE: 1297970341866.1284\n",
      "CatBoost MSE: 1037315083843.0856\n",
      "AdaBoost MSE: 596843601051.4246\n",
      "XGBoost MSE: 751605859342.4871\n",
      "\n",
      "=== Horizon: 7 days ===\n",
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n",
      "Training XGBoost...\n",
      "GradientBoosting MSE: 667507171155.7985\n",
      "LightGBM MSE: 670980038139.1827\n",
      "CatBoost MSE: 881396932754.9519\n",
      "AdaBoost MSE: 584184742271.0957\n",
      "XGBoost MSE: 729046115111.6583\n",
      "\n",
      "=== Horizon: 14 days ===\n",
      "Training GradientBoosting...\n",
      "Training LightGBM...\n",
      "Training CatBoost...\n",
      "Training AdaBoost...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Horizon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhorizon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_train_h, X_test_h, y_train_h, y_test_h \u001b[38;5;241m=\u001b[39m prepare_horizon_data(X, y, horizon)\n\u001b[0;32m----> 9\u001b[0m trained_h \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m horizon_preds \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, pipeline \u001b[38;5;129;01min\u001b[39;00m trained_h\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(models, X_train, y_train)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, pipeline \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     trained_models[name] \u001b[38;5;241m=\u001b[39m pipeline\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_models\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:171\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    168\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:1168\u001b[0m, in \u001b[0;36mAdaBoostRegressor._boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m   1166\u001b[0m X_ \u001b[38;5;241m=\u001b[39m _safe_indexing(X, bootstrap_idx)\n\u001b[1;32m   1167\u001b[0m y_ \u001b[38;5;241m=\u001b[39m _safe_indexing(y, bootstrap_idx)\n\u001b[0;32m-> 1168\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m   1171\u001b[0m error_vect \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(y_predict \u001b[38;5;241m-\u001b[39m y)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/tree/_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ucl_stock_env/lib/python3.10/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_horizon_data = {}\n",
    "y_test_all_horizons = {}\n",
    "model_performances = {}\n",
    "\n",
    "for horizon in horizons:\n",
    "    print(f\"\\n=== Horizon: {horizon} days ===\")\n",
    "    X_train_h, X_test_h, y_train_h, y_test_h = prepare_horizon_data(X, y, horizon)\n",
    "    \n",
    "    trained_h = train_models(models, X_train_h, y_train_h)\n",
    "    \n",
    "    horizon_preds = {}\n",
    "    for name, pipeline in trained_h.items():\n",
    "        preds = pipeline.predict(X_test_h)\n",
    "        horizon_preds[name] = preds\n",
    "    \n",
    "    all_horizon_data[horizon] = horizon_preds\n",
    "    y_test_all_horizons[horizon] = y_test_h\n",
    "    \n",
    "    perf = evaluate_models(trained_h, X_test_h, y_test_h)\n",
    "    model_performances[horizon] = perf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75af779",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION FUNCTIONS (Adapted for single target)\n",
    "# ============================================================\n",
    "\n",
    "def plot_combined_horizons_zoomed(all_horizon_data, y_test_data, zoom_factor=0.15):\n",
    "    horizons = sorted(all_horizon_data.keys())\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        horizon_data = all_horizon_data[horizon]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        \n",
    "        model_colors = {\n",
    "            'GradientBoosting': '#1f77b4',\n",
    "            'LightGBM': '#ff7f0e',\n",
    "            'CatBoost': '#2ca02c',\n",
    "            'AdaBoost': '#d62728',\n",
    "            'XGBoost': '#9467bd'\n",
    "        }\n",
    "        \n",
    "        # Plot actual values\n",
    "        actual = y_test_data[horizon]\n",
    "        ax.plot(actual, label='Actual', linewidth=2.5, color='#333333', alpha=0.7, zorder=5, linestyle='-')\n",
    "        \n",
    "        # Plot model predictions\n",
    "        for model_name, preds in sorted(horizon_data.items()):\n",
    "            color = model_colors.get(model_name, '#999999')\n",
    "            ax.plot(preds, label=model_name, alpha=0.85, linewidth=2, color=color, linestyle='--')\n",
    "        \n",
    "        # Zooming\n",
    "        all_series = [actual] + list(horizon_data.values())\n",
    "        y_min = min(np.min(s) for s in all_series)\n",
    "        y_max = max(np.max(s) for s in all_series)\n",
    "        y_range = y_max - y_min\n",
    "        y_padding = zoom_factor * y_range\n",
    "        ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "        \n",
    "        ax.set_xlim(-50, len(actual) + 50)\n",
    "        \n",
    "        ax.set_title(f'Horizon {horizon} days ahead - Model Comparison (Zoomed)', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Time Steps', fontsize=13)\n",
    "        ax.set_ylabel('Mean Real Estate Price', fontsize=13)\n",
    "        ax.legend(loc='best', fontsize=12, framealpha=0.95, shadow=True, fancybox=True)\n",
    "        ax.grid(True, alpha=0.4, linestyle='--', linewidth=0.8)\n",
    "        ax.set_facecolor('#f8f9fa')\n",
    "        ax.minorticks_on()\n",
    "        ax.grid(which='minor', alpha=0.2, linestyle=':', linewidth=0.5)\n",
    "        ax.yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_residuals(all_horizon_data, y_test_data):\n",
    "    horizons = sorted(all_horizon_data.keys())\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        horizon_data = all_horizon_data[horizon]\n",
    "        actual = y_test_data[horizon]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 6))\n",
    "        \n",
    "        model_colors = {\n",
    "            'GradientBoosting': '#1f77b4',\n",
    "            'LightGBM': '#ff7f0e',\n",
    "            'CatBoost': '#2ca02c',\n",
    "            'AdaBoost': '#d62728',\n",
    "            'XGBoost': '#9467bd'\n",
    "        }\n",
    "        \n",
    "        for model_name, preds in sorted(horizon_data.items()):\n",
    "            residuals = preds - actual\n",
    "            color = model_colors.get(model_name, '#999999')\n",
    "            ax.plot(residuals, label=model_name, alpha=0.8, linewidth=1.5, color=color)\n",
    "        \n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=2, alpha=0.3, label='Zero Error')\n",
    "        \n",
    "        ax.set_title(f'Horizon {horizon} - Prediction Residuals (Prediction - Actual)', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Time Steps', fontsize=13)\n",
    "        ax.set_ylabel('Residual (Prediction Error)', fontsize=13)\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.95)\n",
    "        ax.grid(True, alpha=0.4, linestyle='--')\n",
    "        ax.set_facecolor('#f8f9fa')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_focused_window(all_horizon_data, y_test_data, start_idx=0, window_size=500):\n",
    "    horizons = sorted(all_horizon_data.keys())\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        horizon_data = all_horizon_data[horizon]\n",
    "        actual = y_test_data[horizon]\n",
    "        \n",
    "        end_idx = min(start_idx + window_size, len(actual))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(32, 20))\n",
    "        \n",
    "        model_colors = {\n",
    "            'GradientBoosting': '#1f77b4',\n",
    "            'LightGBM': '#ff7f0e',\n",
    "            'CatBoost': '#2ca02c',\n",
    "            'AdaBoost': '#d62728',\n",
    "            'XGBoost': '#9467bd'\n",
    "        }\n",
    "        \n",
    "        x_range = range(start_idx, end_idx)\n",
    "        ax.plot(x_range, actual[start_idx:end_idx], label='Actual', linewidth=2, color='#333333', alpha=0.5, linestyle='--')\n",
    "        \n",
    "        for model_name, preds in sorted(horizon_data.items()):\n",
    "            color = model_colors.get(model_name, '#999999')\n",
    "            ax.plot(x_range, preds[start_idx:end_idx], label=model_name, alpha=0.9, linewidth=2.5, color=color)\n",
    "        \n",
    "        ax.set_title(f'Horizon {horizon} - Time Steps {start_idx} to {end_idx} (Extreme Zoom)', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Time Steps', fontsize=13)\n",
    "        ax.set_ylabel('Mean Real Estate Price', fontsize=13)\n",
    "        ax.legend(loc='best', fontsize=12, framealpha=0.95)\n",
    "        ax.grid(True, alpha=0.5, linestyle='--')\n",
    "        ax.minorticks_on()\n",
    "        ax.grid(which='minor', alpha=0.25, linestyle=':')\n",
    "        ax.set_facecolor('#f8f9fa')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_combined_horizons_zoomed(all_horizon_data, y_test_all_horizons, zoom_factor=0.005)\n",
    "\n",
    "# To see residuals\n",
    "# plot_residuals(all_horizon_data, y_test_all_horizons)\n",
    "\n",
    "# To see a specific time window with extreme detail\n",
    "# plot_focused_window(all_horizon_data, y_test_all_horizons, start_idx=500, window_size=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl_stock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
