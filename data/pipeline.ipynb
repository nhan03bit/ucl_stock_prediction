{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e7cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports at the top of your notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d3d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modeling_pipeline(data):\n",
    "    \"\"\"\n",
    "    Complete pipeline for real estate price prediction with log transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'price' not in data.columns:\n",
    "        raise ValueError(\"'price' column not found in data\")\n",
    "    \n",
    "    X = data.drop('price', axis=1)\n",
    "    y = data['price']\n",
    "    \n",
    "    # Log transform target and features\n",
    "    y = np.log1p(y)\n",
    "    numerical_features = X.select_dtypes(include=['float64']).columns.tolist()\n",
    "    for col in numerical_features:\n",
    "        X[col] = np.log1p(X[col])\n",
    "    \n",
    "    categorical_features = X.select_dtypes(include=['int64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    print(f\"Total samples: {X.shape[0]}\")\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Original y for metrics\n",
    "    original_y_train = np.expm1(y_train)\n",
    "    original_y_test = np.expm1(y_test)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', RobustScaler(), numerical_features),\n",
    "            ('cat', 'passthrough', categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Models with increased complexity for trees\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0, random_state=42),\n",
    "        'Lasso Regression': Lasso(alpha=0.1, random_state=42),\n",
    "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=200, \n",
    "            max_depth=None, \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=200, \n",
    "            learning_rate=0.1,\n",
    "            max_depth=7,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=9,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostRegressor(\n",
    "            iterations=200,\n",
    "            depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_seed=42,\n",
    "            verbose=0,\n",
    "            cat_features=categorical_features if categorical_features else None\n",
    "        ),\n",
    "        'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5, n_jobs=-1),\n",
    "        'Support Vector Regression': SVR(kernel='linear', C=1.0, epsilon=0.1),  # Changed to linear for speed\n",
    "        'Kernel Ridge Regression': KernelRidge(alpha=1.0, kernel='polynomial', degree=3),\n",
    "    }\n",
    "    \n",
    "    pipelines = {}\n",
    "    for name, model in models.items():\n",
    "        pipelines[name] = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    \n",
    "    # Ensembles with updated base models\n",
    "    estimators = [\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=100, max_depth=6, random_state=42, verbosity=0)),\n",
    "        ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    stacking_reg = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        cv=3\n",
    "    )\n",
    "    \n",
    "    pipelines['Stacking Ensemble'] = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', stacking_reg)\n",
    "    ])\n",
    "    \n",
    "    voting_reg = VotingRegressor([\n",
    "        ('xgb', xgb.XGBRegressor(n_estimators=100, random_state=42, verbosity=0)),\n",
    "        ('lgbm', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    pipelines['Voting Ensemble'] = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', voting_reg)\n",
    "    ])\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, original_y_train, original_y_test, pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64f4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(pipelines, X_train, X_test, y_train, y_test, original_y_train, original_y_test):\n",
    "    \"\"\"\n",
    "    Evaluate all models with metrics on original scale\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    feature_importances = {}\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL COMPARISON AND EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {name}...\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict and back-transform\n",
    "            y_pred_train_log = pipeline.predict(X_train)\n",
    "            y_pred_test_log = pipeline.predict(X_test)\n",
    "            y_pred_train = np.expm1(y_pred_train_log)\n",
    "            y_pred_test = np.expm1(y_pred_test_log)\n",
    "            \n",
    "            # Metrics on original scale\n",
    "            metrics = {\n",
    "                'Model': name,\n",
    "                'Train RMSE': np.sqrt(mean_squared_error(original_y_train, y_pred_train)),\n",
    "                'Test RMSE': np.sqrt(mean_squared_error(original_y_test, y_pred_test)),\n",
    "                'Train MAE': mean_absolute_error(original_y_train, y_pred_train),\n",
    "                'Test MAE': mean_absolute_error(original_y_test, y_pred_test),\n",
    "                'Train R²': r2_score(original_y_train, y_pred_train),\n",
    "                'Test R²': r2_score(original_y_test, y_pred_test),\n",
    "                'Test MAPE': mean_absolute_percentage_error(original_y_test, y_pred_test)\n",
    "            }\n",
    "            \n",
    "            # CV on log scale, but can adjust if needed\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                                        cv=5, scoring='neg_root_mean_squared_error')\n",
    "            metrics['CV RMSE (Mean)'] = -cv_scores.mean()  # on log scale\n",
    "            metrics['CV RMSE (Std)'] = cv_scores.std()\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            print(f\"\\n{name} Performance (original scale):\")\n",
    "            print(f\"  Test RMSE: {metrics['Test RMSE']:.2f}\")\n",
    "            print(f\"  Test MAE: {metrics['Test MAE']:.2f}\")\n",
    "            print(f\"  Test R²: {metrics['Test R²']:.3f}\")\n",
    "            print(f\"  Cross-validation RMSE (log scale): {metrics['CV RMSE (Mean)']:.2f} (+/- {metrics['CV RMSE (Std)']:.2f})\")\n",
    "            \n",
    "            if hasattr(pipeline.named_steps['model'], 'feature_importances_'):\n",
    "                importances = pipeline.named_steps['model'].feature_importances_\n",
    "                if hasattr(pipeline.named_steps['preprocessor'], 'get_feature_names_out'):\n",
    "                    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "                else:\n",
    "                    feature_names = X_train.columns\n",
    "                \n",
    "                feature_importances[name] = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(f\"\\n  Top 5 Important Features:\")\n",
    "                for i, (feat, imp) in enumerate(feature_importances[name].head().values):\n",
    "                    print(f\"    {i+1}. {feat}: {imp:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('Test RMSE')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON SUMMARY (Sorted by Test RMSE)\")\n",
    "    print(\"=\"*80)\n",
    "    display_cols = ['Model', 'Test RMSE', 'Test MAE', 'Test R²', 'CV RMSE (Mean)', 'CV RMSE (Std)']\n",
    "    print(results_df[display_cols].to_string(index=False))\n",
    "    \n",
    "    return results_df, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b65b4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(pipelines, X_train, y_train, model_name='LightGBM'):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for a specific model\n",
    "    \"\"\"\n",
    "    if model_name not in pipelines:\n",
    "        print(f\"Model {model_name} not found in pipelines\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HYPERPARAMETER TUNING FOR {model_name.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Define parameter grids for different models\n",
    "    param_grids = {\n",
    "        'LightGBM': {\n",
    "            'model__n_estimators': [100, 200, 300, 500, 1000],\n",
    "            'model__max_depth': [3, 5, 7, 9, 11, -1],\n",
    "            'model__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "            'model__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'model__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'model__reg_alpha': [0, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "            'model__reg_lambda': [0, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "            'model__num_leaves': [31, 63, 127, 255]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model__n_estimators': [100, 200, 300, 500],\n",
    "            'model__max_depth': [3, 5, 7, 9, 11],\n",
    "            'model__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "            'model__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'model__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "            'model__gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "            'model__reg_alpha': [0, 0.001, 0.01, 0.1],\n",
    "            'model__reg_lambda': [0.5, 1, 1.5, 2]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'model__n_estimators': [50, 100, 200, 300],\n",
    "            'model__max_depth': [5, 10, 15, 20, None],\n",
    "            'model__min_samples_split': [2, 5, 10, 20],\n",
    "            'model__min_samples_leaf': [1, 2, 4, 8],\n",
    "            'model__max_features': ['sqrt', 'log2', None, 0.5, 0.7]\n",
    "        },\n",
    "        'CatBoost': {\n",
    "            'model__iterations': [100, 200, 300, 500],\n",
    "            'model__depth': [4, 6, 8, 10, 12],\n",
    "            'model__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "            'model__l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "            'model__subsample': [0.6, 0.8, 1.0],\n",
    "            'model__colsample_bylevel': [0.6, 0.8, 1.0]\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model__n_estimators': [100, 200, 300, 500],\n",
    "            'model__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 7, 9],\n",
    "            'model__subsample': [0.6, 0.8, 1.0],\n",
    "            'model__min_samples_split': [2, 5, 10],\n",
    "            'model__min_samples_leaf': [1, 2, 4],\n",
    "            'model__max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "        'Support Vector Regression': {\n",
    "            'model__kernel': ['linear', 'rbf'],\n",
    "            'model__C': [0.1, 1, 10, 100],\n",
    "            'model__epsilon': [0.01, 0.1, 0.5, 1],\n",
    "            'model__gamma': ['scale', 'auto']\n",
    "        },\n",
    "        'K-Nearest Neighbors': {\n",
    "            'model__n_neighbors': [3, 5, 7, 10, 15],\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__p': [1, 2],\n",
    "            'model__leaf_size': [10, 30, 50]\n",
    "        },\n",
    "        'Kernel Ridge Regression': {\n",
    "            'model__alpha': [0.01, 0.1, 1, 10],\n",
    "            'model__kernel': ['linear', 'polynomial', 'rbf'],\n",
    "            'model__degree': [2, 3, 4],\n",
    "            'model__gamma': [0.01, 0.1, 1, None]\n",
    "        },\n",
    "        'Ridge Regression': {\n",
    "            'model__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "        },\n",
    "        'Lasso Regression': {\n",
    "            'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'model__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if model_name not in param_grids:\n",
    "        print(f\"No parameter grid defined for {model_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Perform randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipelines[model_name],\n",
    "        param_grids[model_name],\n",
    "        n_iter=50,  # Increased for better search\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,  # Increased folds for better validation\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\nBest parameters for {model_name}:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest cross-validation RMSE: {-random_search.best_score_:.2f}\")\n",
    "    \n",
    "    return random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "995382af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_model(best_model_name, pipelines, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Create and evaluate the final selected model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL MODEL: {best_model_name.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get the model pipeline\n",
    "    final_pipeline = pipelines[best_model_name]\n",
    "    \n",
    "    # Optionally perform hyperparameter tuning\n",
    "    tuned_model = hyperparameter_tuning(pipelines, X_train, y_train, best_model_name)\n",
    "    \n",
    "    if tuned_model:\n",
    "        final_pipeline = tuned_model\n",
    "    \n",
    "    # Fit the final model on all training data\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = final_pipeline.predict(X_train)\n",
    "    y_pred_test = final_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nFinal Model Performance:\")\n",
    "    print(f\"  Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"  Testing RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"  Training R²: {train_r2:.3f}\")\n",
    "    print(f\"  Testing R²: {test_r2:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(final_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "        importances = final_pipeline.named_steps['model'].feature_importances_\n",
    "        if hasattr(final_pipeline.named_steps['preprocessor'], 'get_feature_names_out'):\n",
    "            feature_names = final_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "        else:\n",
    "            feature_names = X_train.columns\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(feature_importance_df.head(20)['feature'][::-1], \n",
    "                feature_importance_df.head(20)['importance'][::-1])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Feature Importance - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return final_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0b92689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(model, new_data, scaler=None):\n",
    "    \"\"\"\n",
    "    Make predictions on new data\n",
    "    \"\"\"\n",
    "    predictions = model.predict(new_data)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542ee07",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_complete_modeling_pipeline(data):\n",
    "    \"\"\"\n",
    "    Run the complete modeling pipeline\n",
    "    \"\"\"\n",
    "    print(\"STARTING COMPLETE MODELING PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create modeling pipeline\n",
    "    X_train, X_test, y_train, y_test, original_y_train, original_y_test, pipelines = create_modeling_pipeline(data)\n",
    "    \n",
    "    # Evaluate all models\n",
    "    results_df, feature_importances = evaluate_models(pipelines, X_train, X_test, y_train, y_test, original_y_train, original_y_test)\n",
    "    \n",
    "    # Select best model based on Test RMSE\n",
    "    best_model_name = results_df.iloc[0]['Model']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SELECTED BEST MODEL: {best_model_name}\")\n",
    "    print(f\"Test RMSE: {results_df.iloc[0]['Test RMSE']:.2f}\")\n",
    "    print(f\"Test R²: {results_df.iloc[0]['Test R²']:.3f}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Create and save final model\n",
    "    final_model = create_final_model(best_model_name, pipelines, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Return everything for further analysis\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'results': results_df,\n",
    "        'feature_importances': feature_importances,\n",
    "        'final_model': final_model,\n",
    "        'best_model_name': best_model_name,\n",
    "        'all_pipelines': pipelines\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (354089, 5)\n",
      "Columns: ['price', 'postcode_mean_price', 'street_mean_price', 'town_city_mean_price', 'district_mean_price']\n",
      "\n",
      "Starting model training pipeline...\n",
      "STARTING COMPLETE MODELING PIPELINE\n",
      "============================================================\n",
      "Number of features: 4\n",
      "Categorical features: 0\n",
      "Numerical features: 4\n",
      "Total samples: 354089\n",
      "================================================================================\n",
      "MODEL COMPARISON AND EVALUATION\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Training Linear Regression...\n",
      "============================================================\n",
      "\n",
      "Linear Regression Performance (original scale):\n",
      "  Test RMSE: 223704.30\n",
      "  Test MAE: 24588.65\n",
      "  Test R²: 0.835\n",
      "  Cross-validation RMSE (log scale): 0.19 (+/- 0.01)\n",
      "\n",
      "============================================================\n",
      "Training Ridge Regression...\n",
      "============================================================\n",
      "\n",
      "Ridge Regression Performance (original scale):\n",
      "  Test RMSE: 223703.88\n",
      "  Test MAE: 24589.54\n",
      "  Test R²: 0.835\n",
      "  Cross-validation RMSE (log scale): 0.19 (+/- 0.01)\n",
      "\n",
      "============================================================\n",
      "Training Lasso Regression...\n",
      "============================================================\n",
      "\n",
      "Lasso Regression Performance (original scale):\n",
      "  Test RMSE: 337239.89\n",
      "  Test MAE: 55989.38\n",
      "  Test R²: 0.624\n",
      "  Cross-validation RMSE (log scale): 0.22 (+/- 0.01)\n",
      "\n",
      "============================================================\n",
      "Training ElasticNet...\n",
      "============================================================\n",
      "\n",
      "ElasticNet Performance (original scale):\n",
      "  Test RMSE: 314447.76\n",
      "  Test MAE: 50504.06\n",
      "  Test R²: 0.673\n",
      "  Cross-validation RMSE (log scale): 0.21 (+/- 0.01)\n",
      "\n",
      "============================================================\n",
      "Training Random Forest...\n",
      "============================================================\n",
      "\n",
      "Random Forest Performance (original scale):\n",
      "  Test RMSE: 170014.56\n",
      "  Test MAE: 30987.41\n",
      "  Test R²: 0.905\n",
      "  Cross-validation RMSE (log scale): 0.23 (+/- 0.00)\n",
      "\n",
      "  Top 5 Important Features:\n",
      "    1. num__postcode_mean_price: 0.9668\n",
      "    2. num__street_mean_price: 0.0174\n",
      "    3. num__district_mean_price: 0.0079\n",
      "    4. num__town_city_mean_price: 0.0079\n",
      "\n",
      "============================================================\n",
      "Training Gradient Boosting...\n",
      "============================================================\n",
      "\n",
      "Gradient Boosting Performance (original scale):\n",
      "  Test RMSE: 167752.64\n",
      "  Test MAE: 23805.64\n",
      "  Test R²: 0.907\n",
      "  Cross-validation RMSE (log scale): 0.18 (+/- 0.01)\n",
      "\n",
      "  Top 5 Important Features:\n",
      "    1. num__postcode_mean_price: 0.9893\n",
      "    2. num__street_mean_price: 0.0077\n",
      "    3. num__district_mean_price: 0.0017\n",
      "    4. num__town_city_mean_price: 0.0013\n",
      "\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "============================================================\n",
      "\n",
      "XGBoost Performance (original scale):\n",
      "  Test RMSE: 363812.37\n",
      "  Test MAE: 31787.90\n",
      "  Test R²: 0.563\n",
      "  Cross-validation RMSE (log scale): 0.20 (+/- 0.00)\n",
      "\n",
      "  Top 5 Important Features:\n",
      "    1. num__postcode_mean_price: 0.9636\n",
      "    2. num__street_mean_price: 0.0181\n",
      "    3. num__town_city_mean_price: 0.0112\n",
      "    4. num__district_mean_price: 0.0072\n",
      "\n",
      "============================================================\n",
      "Training LightGBM...\n",
      "============================================================\n",
      "\n",
      "LightGBM Performance (original scale):\n",
      "  Test RMSE: 407873.80\n",
      "  Test MAE: 30440.07\n",
      "  Test R²: 0.450\n",
      "  Cross-validation RMSE (log scale): 0.19 (+/- 0.00)\n",
      "\n",
      "  Top 5 Important Features:\n",
      "    1. num__postcode_mean_price: 1554.0000\n",
      "    2. num__street_mean_price: 1517.0000\n",
      "    3. num__district_mean_price: 1483.0000\n",
      "    4. num__town_city_mean_price: 1446.0000\n",
      "\n",
      "============================================================\n",
      "Training CatBoost...\n",
      "============================================================\n",
      "\n",
      "CatBoost Performance (original scale):\n",
      "  Test RMSE: 414501.55\n",
      "  Test MAE: 31441.00\n",
      "  Test R²: 0.432\n",
      "  Cross-validation RMSE (log scale): 0.19 (+/- 0.01)\n",
      "\n",
      "  Top 5 Important Features:\n",
      "    1. num__postcode_mean_price: 92.9603\n",
      "    2. num__street_mean_price: 3.9920\n",
      "    3. num__town_city_mean_price: 1.5344\n",
      "    4. num__district_mean_price: 1.5133\n",
      "\n",
      "============================================================\n",
      "Training K-Nearest Neighbors...\n",
      "============================================================\n",
      "\n",
      "K-Nearest Neighbors Performance (original scale):\n",
      "  Test RMSE: 270694.99\n",
      "  Test MAE: 32960.02\n",
      "  Test R²: 0.758\n",
      "  Cross-validation RMSE (log scale): 0.20 (+/- 0.00)\n",
      "\n",
      "============================================================\n",
      "Training Support Vector Regression...\n",
      "============================================================\n",
      "\n",
      "Support Vector Regression Performance (original scale):\n",
      "  Test RMSE: 223190.71\n",
      "  Test MAE: 23217.67\n",
      "  Test R²: 0.835\n",
      "  Cross-validation RMSE (log scale): 0.19 (+/- 0.01)\n",
      "\n",
      "============================================================\n",
      "Training Kernel Ridge Regression...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data and run complete pipeline\n",
    "data = pd.read_csv('preprocessed_real_estate_high_corr.csv')\n",
    "\n",
    "print(f\"Loaded data shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "print(f\"\\nStarting model training pipeline...\")\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = run_complete_modeling_pipeline(data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_by_feature(X_test, y_true, y_pred, feature_name, \n",
    "                              n_bins=10, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot predictions grouped by a specific feature\n",
    "    \"\"\"\n",
    "    if feature_name not in X_test.columns:\n",
    "        print(f\"Feature '{feature_name}' not found in X_test\")\n",
    "        return\n",
    "    \n",
    "    feature_values = X_test[feature_name]\n",
    "    \n",
    "    # Create bins if numeric\n",
    "    if pd.api.types.is_numeric_dtype(feature_values):\n",
    "        bins = pd.cut(feature_values, bins=n_bins)\n",
    "    else:\n",
    "        bins = feature_values\n",
    "    \n",
    "    # Calculate metrics per bin\n",
    "    results = []\n",
    "    for bin_name, group_idx in bins.groupby(bins).groups.items():\n",
    "        group_y_true = y_true.iloc[group_idx] if hasattr(y_true, 'iloc') else y_true[group_idx]\n",
    "        group_y_pred = y_pred.iloc[group_idx] if hasattr(y_pred, 'iloc') else y_pred[group_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(group_y_true, group_y_pred))\n",
    "        mae = mean_absolute_error(group_y_true, group_y_pred)\n",
    "        r2 = r2_score(group_y_true, group_y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'bin': bin_name,\n",
    "            'count': len(group_idx),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'avg_true': group_y_true.mean(),\n",
    "            'avg_pred': group_y_pred.mean()\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # 1. RMSE by feature bin\n",
    "    axes[0, 0].bar(range(len(results_df)), results_df['rmse'])\n",
    "    axes[0, 0].set_xlabel(f'{feature_name} Bins')\n",
    "    axes[0, 0].set_ylabel('RMSE')\n",
    "    axes[0, 0].set_title(f'RMSE by {feature_name}')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Sample count by bin\n",
    "    axes[0, 1].bar(range(len(results_df)), results_df['count'])\n",
    "    axes[0, 1].set_xlabel(f'{feature_name} Bins')\n",
    "    axes[0, 1].set_ylabel('Sample Count')\n",
    "    axes[0, 1].set_title(f'Sample Distribution by {feature_name}')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Average Actual vs Predicted\n",
    "    x_pos = range(len(results_df))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar([p - width/2 for p in x_pos], results_df['avg_true'], \n",
    "                   width, label='Actual', alpha=0.7)\n",
    "    axes[1, 0].bar([p + width/2 for p in x_pos], results_df['avg_pred'], \n",
    "                   width, label='Predicted', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel(f'{feature_name} Bins')\n",
    "    axes[1, 0].set_ylabel('Average Price')\n",
    "    axes[1, 0].set_title(f'Average Actual vs Predicted by {feature_name}')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. R² by bin\n",
    "    axes[1, 1].bar(range(len(results_df)), results_df['r2'])\n",
    "    axes[1, 1].set_xlabel(f'{feature_name} Bins')\n",
    "    axes[1, 1].set_ylabel('R² Score')\n",
    "    axes[1, 1].set_title(f'R² Score by {feature_name}')\n",
    "    axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle(f'Prediction Analysis by {feature_name}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def plot_error_by_magnitude(y_true, y_pred, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plot error metrics by actual value magnitude\n",
    "    \"\"\"\n",
    "    # Bin by actual value magnitude\n",
    "    bins = pd.qcut(y_true, q=n_bins, duplicates='drop')\n",
    "    \n",
    "    results = []\n",
    "    for bin_name, group_idx in bins.groupby(bins).groups.items():\n",
    "        group_y_true = y_true.iloc[group_idx] if hasattr(y_true, 'iloc') else y_true[group_idx]\n",
    "        group_y_pred = y_pred.iloc[group_idx] if hasattr(y_pred, 'iloc') else y_pred[group_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(group_y_true, group_y_pred))\n",
    "        mae = mean_absolute_error(group_y_true, group_y_pred)\n",
    "        mape = np.mean(np.abs((group_y_true - group_y_pred) / group_y_true)) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'price_range': bin_name,\n",
    "            'avg_price': group_y_true.mean(),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'count': len(group_idx)\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. RMSE vs Price Range\n",
    "    axes[0, 0].plot(results_df['avg_price'], results_df['rmse'], 'bo-')\n",
    "    axes[0, 0].set_xlabel('Average Price in Bin')\n",
    "    axes[0, 0].set_ylabel('RMSE')\n",
    "    axes[0, 0].set_title('RMSE by Price Range')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. MAE vs Price Range\n",
    "    axes[0, 1].plot(results_df['avg_price'], results_df['mae'], 'ro-')\n",
    "    axes[0, 1].set_xlabel('Average Price in Bin')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].set_title('MAE by Price Range')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. MAPE vs Price Range\n",
    "    axes[1, 0].plot(results_df['avg_price'], results_df['mape'], 'go-')\n",
    "    axes[1, 0].set_xlabel('Average Price in Bin')\n",
    "    axes[1, 0].set_ylabel('MAPE (%)')\n",
    "    axes[1, 0].set_title('Mean Absolute Percentage Error by Price Range')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sample Distribution\n",
    "    axes[1, 1].bar(range(len(results_df)), results_df['count'])\n",
    "    axes[1, 1].set_xlabel('Price Range Bin')\n",
    "    axes[1, 1].set_ylabel('Sample Count')\n",
    "    axes[1, 1].set_title('Sample Distribution by Price Range')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.suptitle('Error Analysis by Price Magnitude', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucl_stock_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
